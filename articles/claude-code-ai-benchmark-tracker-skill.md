---
title: "Claude Codeで「AIモデル比較レポート」を自動生成するスキルを作った"
emoji: "📊"
type: "tech"
topics: ["ClaudeCode", "Claude", "AI", "Skills", "個人開発"]
published: true
---

## はじめに

「今、コーディングに一番強いAIは？」「画像生成ならどれがいい？」「推論モデルのベンチマーク1位は？」

こうした質問に答えるには、Chatbot Arena、SWE-bench、MTEB、LM Arena など複数のリーダーボードを巡回し、最新のスコアと料金を突き合わせる必要がある。手動でやると半日仕事だ。

そこで Claude Code の Skills を使い、**スラッシュコマンド一発で最新のAIモデル比較レポートを生成するスキル**を作った。出力は「AI初心者にも分かるおすすめカード」と「エンジニア向けベンチマーク比較テーブル」の2ビュー切り替えができるインタラクティブHTMLだ。

この記事では、スキルの設計・実装・実行結果と、「2つの異なる読者」に同時に応える出力をどう設計したかを紹介する。

:::message
この記事は [indie-idea-scout スキルの記事](https://zenn.dev/s4kura/articles/claude-code-indie-idea-scout-skill) の続編にあたる。Skills の基本は前回の記事を参照。
:::

## 何を作ったか — スキルの全体像

作成したスキルは `ai-benchmark-tracker`。以下のファイル構成で `~/.claude/skills/` に配置した。

```
ai-benchmark-tracker/
├── SKILL.md                    # スキル定義（4フェーズの実行手順）
└── references/
    ├── categories.md           # 8カテゴリの定義・調査項目
    ├── search-strategy.md      # WebSearchクエリ戦略
    └── output-format.md        # HTML出力仕様（デザイン・データ構造）
```

`/ai-benchmark-tracker` と入力するだけで、以下が自動実行される。

```
Phase 1: リサーチ（WebSearch 12-18回 + WebFetch 5-10回）
    ↓  8カテゴリ × 複数ソースで最新データ収集
Phase 2: 分析・整理
    ↓  カテゴリ別ランキング + データ構造化
Phase 3: HTML生成
    ↓  2ビュー切り替えのインタラクティブHTML
Phase 4: 出力
    → ai-benchmark-report-{YYYY-MM-DD}.html
```

対象は全8カテゴリ。LLM、コーディング、画像生成、動画生成、音声・音楽、マルチモーダル、推論・数学、エンベディング。

## 設計のポイント — 「2つの読者」問題

このスキルの最大の設計課題は、**1つのレポートで2つの全く異なる読者に応える**ことだった。

- **一般向け**: 「このAIは何が得意？料金は？日本語使える？」が知りたい
- **エンジニア向け**: 「SWE-bench何%？APIの入力トークン単価は？コンテキスト長は？」が知りたい

最初は2つの別レポートを生成する案もあった。しかし情報の重複が多い上に、「一般向けを読んでから詳細を知りたくなった」場合にファイルを行き来するのは不便だ。

解決策は **1つのHTMLに2ビュー切り替え**を埋め込むこと。ヘッダーに「一般向け / エンジニア向け」のトグルボタンを置き、同じデータを異なるUI（カード vs テーブル）で表示する。

```
一般向けビュー:
  ┌──────────────────────────┐
  │ 1位  Gemini 3 Pro  [New] │
  │ 万能型の最高性能モデル     │
  │ ✅ 得意: MMLU-Pro最高水準  │
  │ 💰 料金: $5.2/100万トークン│
  │ 🌐 日本語: ◎              │
  └──────────────────────────┘

エンジニア向けビュー:
  ┌─────────┬────────┬──────┬───────┐
  │ Model   │ Elo    │ MMLU │ Price │
  ├─────────┼────────┼──────┼───────┤
  │ Gemini  │ 1492   │ 89.8 │ $5.2  │
  │ Claude  │ 1490   │ 85.1 │ $15   │
  └─────────┴────────┴──────┴───────┘
```

これを実現するため、HTML内にデータをJSON形式で埋め込み、JavaScriptでビュー切り替え・カテゴリナビゲーション・テーブルソートを処理する構造にした。

### output-format.md がHTML品質を決める

`output-format.md` には以下を定義した。

- **カラーパレット** — ダークモードデフォルト（`#0f1117` 背景）、ライトモード対応
- **データ構造** — 内部JSONのスキーマ（general / technical の二層構造）
- **インタラクション** — ソート可能テーブル、折りたたみパネル、ツールチップ
- **レスポンシブ** — デスクトップ3列、タブレット2列、モバイル1列

この仕様がないと、Claude は毎回異なるデザインのHTMLを生成する。色、レイアウト、インタラクションが安定しない。**デザイン仕様を参照ファイルに固定する**ことで、何度実行しても同じ品質のレポートが出力される。

## categories.md — 8カテゴリで「何を調べるか」を定義する

各カテゴリで「一般向け」と「エンジニア向け」の両方の調査項目を定義した。

```markdown:references/categories.md（LLMカテゴリの抜粋）
### 一般向け調査項目
- 得意なこと（文章作成、翻訳、要約、質問応答 etc.）
- 初心者へのおすすめモデル
- 無料で使えるか / 料金感（月額○○円程度）
- 日本語の対応度

### エンジニア向け調査項目
- **ベンチマーク**: MMLU, MMLU-Pro, GPQA Diamond, Arena Elo
- **API仕様**: コンテキスト長、出力トークン上限
- **料金**: 入力/出力トークン単価
- **レイテンシ**: TTFT, TPS（tokens/sec）
- **特殊機能**: Function Calling, JSON Mode, Vision対応
```

8カテゴリ全てでこの「二層構造」を定義している。これにより Claude は一般向け情報（「得意なこと」「おすすめ」）とエンジニア向け情報（ベンチマークスコア、API仕様）の両方を確実に収集する。

## search-strategy.md — 検索の「型」を8カテゴリ分用意する

前回の indie-idea-scout で学んだ教訓がここで活きた。**クエリテンプレートをカテゴリ別に用意すると、検索の網羅性が変わる**。

```markdown:references/search-strategy.md（LLMカテゴリの抜粋）
### 1. LLM（テキスト生成・対話）
"LLM benchmark comparison" {year}
"Chatbot Arena" leaderboard {year}
"MMLU" "GPQA" best LLM model {year}
LLM model comparison API pricing {year}
大規模言語モデル 比較 ランキング {year}年
```

全8カテゴリで英語・日本語のクエリを用意し、合計検索回数の目安も明記した。

| フェーズ | WebSearch | WebFetch |
|---------|-----------|----------|
| LLM + コーディング | 3-4回 | 2-3回 |
| 画像 + 動画生成 | 3-4回 | 1-2回 |
| 音声 + マルチモーダル | 2-3回 | 1-2回 |
| 推論 + エンベディング | 2-3回 | 1-2回 |
| 補足調査 | 2-4回 | 0-1回 |
| **合計** | **12-18回** | **5-10回** |

検索回数の目安を明示しているのがポイントだ。これがないと Claude は3-4回の検索で「十分調べた」と判断して Phase 2 に移ってしまう。目安があると「まだ画像生成の調査が足りない」と自分で判断して追加検索する。

### ソースの信頼性も定義する

```markdown:references/search-strategy.md（信頼性テーブル抜粋）
| 信頼性 | ソース |
|--------|--------|
| 最高 | 公式リーダーボード（LMSYS Arena, MTEB, SWE-bench） |
| 高   | Artificial Analysis, 査読付き論文, 公式ドキュメント |
| 中   | TechCrunch, Ars Technica, 技術ブログ |
| 低   | 個人ブログ（単独では使わない）、SNS投稿 |
```

信頼性のランクを定義しておくと、Claude は低信頼ソースの情報を高信頼ソースで裏付けてから採用するようになる。ベンチマークスコアのような定量データは特に、出典の信頼性が重要だ。

## モデル選定基準 — 「漏れ」を防ぐルール

スキルの初回実行で問題が発覚した。**ベンチマーク上位のモデルだけを拾い、人気モデルや話題のモデルが漏れる**。たとえば画像生成でGeminiが入っていなかった。コーディングでCodexが抜けていた。

原因は、SKILL.md にモデル選定基準がなかったこと。Claude はベンチマークリーダーボードの上位だけを見て、それ以外の有名モデルを無視していた。

修正として、3つの選定基準を明示した。

```markdown:SKILL.md（モデル選定基準の抜粋）
## モデル選定基準

各カテゴリで以下の条件を満たすモデルを**すべて**含める。

1. **ベンチマーク上位**: リーダーボードでトップ5-8に入るモデル
2. **常に人気**: ユーザー数・知名度が高い定番モデル
3. **話題のモデル**: 直近3ヶ月以内にリリースされ注目を集めているモデル
```

さらに、直近3ヶ月以内のモデルには **「New」バッジ** を付けるルールも追加した。レポートを見たとき「何が最近出たモデルなのか」が一目で分かる。

:::message
スキルの品質は「初回実行の失敗」から改善される。最初から完璧を目指さず、実行→確認→SKILL.md修正のサイクルを回すのが近道だ。
:::

## 実行結果 — 2026年2月17日時点のレポート

実際に実行した結果の一部を紹介する。WebSearch 15回以上 + WebFetch でデータを収集し、8カテゴリ・49モデルのレポートが生成された。

### 各カテゴリの1位

| カテゴリ | 1位モデル | 根拠 |
|---------|----------|------|
| LLM | Gemini 3 Pro | Arena Elo 1492, MMLU-Pro 89.8% |
| コーディング | Claude Code | SWE-bench 80.8% |
| 画像生成 | GPT Image 1.5 | LM Arena ELO 1264 |
| 動画生成 | Veo 3.1 | MovieGenBench総合1位 |
| 音声・音楽 | Fish Audio S1 | TTS-Arena ELO 1339 |
| マルチモーダル | Gemini 3 Pro | MMMU-Pro 81% |
| 推論・数学 | o3 | AIME 2025 98.4% |
| エンベディング | Qwen3-Embedding-8B | MTEB多言語 70.58 |

### 注目の発見

レポートから読み取れた特筆すべきポイント。

- **LLM**: Grok 4.1 が入力$0.20/100万トークンという破格で4位。Gemini 3 Pro（$5.2）やClaude Opus 4.6（$15）と比べると圧倒的に安い
- **コーディング**: Claude Code（SWE-bench優位） vs OpenAI Codex（Terminal-Bench優位）で得意領域が明確に分かれている
- **画像生成**: GPT Image 1.5 が ELO 1264 で Midjourney を抜いて首位。テキスト描画精度が決め手
- **推論**: o4-mini が AIME 2025 99.5% で o3（98.4%）を超えた。コスト効率の高い推論モデルが台頭

### HTML出力の見た目

生成されたHTMLは単一ファイルで外部依存なし。CSS/JSは全て埋め込まれている。

一般向けビューではカード形式で「得意なこと」「料金」「日本語対応」「おすすめユーザー」が表示される。エンジニア向けビューに切り替えると、ベンチマーク比較テーブル（ソート可能・棒グラフ付き）、スペック表、機能詳細パネルが表示される。

**一般向けビュー** — カード形式でモデルの強み・料金・日本語対応を表示
![一般向けビュー](/images/ai-benchmark-general-view.png)

**エンジニア向けビュー** — ベンチマーク比較テーブル（ソート可能・棒グラフ付き）
![エンジニア向けビュー](/images/ai-benchmark-engineer-view.png)

## indie-idea-scout との設計パターンの比較

前回の indie-idea-scout と今回の ai-benchmark-tracker は、同じ「WebSearch リサーチスキル」でありながら出力の性質が異なる。

| 比較項目 | indie-idea-scout | ai-benchmark-tracker |
|---------|-----------------|---------------------|
| 出力形式 | Markdown レポート | インタラクティブHTML |
| 読者 | 自分（個人開発者） | 2種類（一般 + エンジニア） |
| 評価方法 | 5軸スコアリング | ベンチマーク順位 + おすすめ度 |
| references/ | 3ファイル | 3ファイル |
| 検索回数 | 10-14回 | 12-18回 |

共通する設計原則は同じだ。

1. **SKILL.md はワークフロー、references/ は判断基準**
2. **検索はクエリテンプレートで制御する**
3. **出力フォーマットを仕様として固定する**

違いは「出力の複雑さ」。HTML出力の場合、`output-format.md` にデザイン仕様・データ構造・インタラクション仕様を書く必要がある。これが今回のスキルで最も手間がかかった部分だった。逆に言えば、Markdown出力で十分なスキルなら references/ は2ファイルで済む。

## まとめ

「今どのAIが一番優秀か」を定期的に確認するタスクを、Claude Code のスキルとして自動化した。

設計から得た知見をまとめる。

- **2つの読者に1つのレポートで応える** — データをJSON化し、ビュー切り替えで表示を分岐させる
- **モデル選定基準は明示する** — 「ベンチマーク上位」だけでなく「人気」「話題」の3軸で漏れを防ぐ
- **HTML出力にはデザイン仕様が必須** — output-format.md にカラーパレット・レイアウト・データ構造を固定しないと毎回違うものが出る
- **初回実行の失敗がスキルを磨く** — モデルの漏れ、ランキングの誤りは実行して初めて分かる。修正→再実行のサイクルが品質を上げる

スキルのファイル構成は SKILL.md + references/ 3ファイルで合計約25KB。一度作れば `/ai-benchmark-tracker` と打つだけで、その時点の最新データに基づいた49モデル・8カテゴリの比較レポートが得られる。

月に一度実行すれば、AIモデルの勢力図がどう変わっているかを定点観測できる。技術選定のたびにリーダーボードを巡回する手間から解放される。
