---
title: "バイブコーディングの「スパゲッティコード」をAIで検出する — Claude Code自作スキルの設計と苦戦"
emoji: "🩺"
type: "tech"
topics: ["ClaudeCode", "Claude", "AI", "コード品質", "個人開発"]
published: true
---

## はじめに

バイブコーディングで生成されたコードは、動くけど読めない。関数が300行、1ファイルに5つの責務、循環参照だらけ。ESLint は構文エラーを教えてくれるが「この関数、やりすぎでは？」とは言ってくれない。

そこで Claude Code の Skills を使い、**コードの品質を「健康診断」メタファーで可視化するスキル**を作った。名前は **CodeVital**。3つの軸でスコアリングし、0-100の CodeVital Score として出力する。

![CodeVital HTMLレポート — 概要タブ](/images/codevital-report-overview.png)

この記事では、CodeVital の設計思想・実装中に直面した課題と解決策・現状の限界・今後の計画を紹介する。

## CodeVital の全体像

### コンセプト: ESLint ではできないこと

ESLint やSonarQube は「ルール違反の検出」が仕事だ。`no-unused-vars` や `max-lines-per-function` といった定量的なルールベースの分析。

CodeVital がやりたいのは**意味レベルの分析**だ。

```
ESLint:  「この関数は300行です」
CodeVital: 「この関数はバリデーション・決済・DB保存・通知送信の4責務を担っています。
           価格計算とPaymentServiceに分離することを提案します」
```

Claude の言語理解力を活かし、「何が問題か」だけでなく「なぜ問題か」「どう直すか」まで提示する。

### 3つの診断軸

| 軸 | 配点 | 計測対象 |
|----|------|----------|
| **Complexity（複雑度）** | 40% | 関数行数、ネスト深度、分岐数、パラメータ数、認知的複雑度 |
| **Coupling（結合度）** | 30% | import数、被import数、循環参照、外部依存、意味的結合 |
| **Responsibility（責務）** | 30% | 関数の役割数、関心事数、export数、命名一貫性 |

Complexity を40%にした理由は明確で、**最もアクション可能**だから。関数を分割するだけでスコアが上がる。Coupling や Responsibility は設計変更を伴うため、まず Complexity から手をつけるのが現実的。

### スコアリング

```
CodeVital Score = Complexity × 0.40 + Coupling × 0.30 + Responsibility × 0.30
```

| ランク | スコア | 意味 |
|--------|--------|------|
| Healthy | 80-100 | 良好。軽微な改善のみ |
| Warning | 50-79 | 改善の余地あり |
| Critical | 0-49 | 早急な改善が必要 |

### ファイル構成

```
~/.claude/skills/codevital/
├── SKILL.md                    # スキル定義（実行手順のオーケストレーター）
└── references/
    ├── analysis-criteria.md    # 3軸の詳細な診断基準・閾値
    ├── output-format.md        # 出力仕様（Quick/Report両モード）
    └── report-template.html    # HTMLテンプレート（CSS/JS込み）
```

`/codevital` と入力すると対話的に分析対象と出力モードを選択し、診断が始まる。

## 出力: 2つのモード

### Quick モード（ターミナル出力）

サッと確認したいとき用。マークダウン形式でスコア・要注意ファイル Top5・改善提案をターミナルに直接出力する。

![CodeVital Quick モード — ターミナル出力](/images/codevital-quick-terminal.png)

### Report モード（HTML レポート）

じっくり確認・チームで共有したいとき用。ダークモード対応のインタラクティブ HTML を生成する。

![CodeVital Report モード — 概要タブ](/images/codevital-report-overview.png)

3つのタブで構成:
- **概要**: 総合スコア・3軸カード・健康分布・Top5要注意ファイル・良い点
- **ファイル一覧**: ソート可能なテーブル。行クリックで所見を展開
- **所見**: 重要度別グルーピング。軸フィルター付き

![CodeVital ファイル一覧タブ — 所見の展開](/images/codevital-filelist-detail.png)

## 実装で苦戦したこと

ここからが本題。設計段階では想定しなかった課題が次々と出てきた。

### 苦戦1: 分析が遅い

最初の実装はシンプルだった。全ファイルを1つずつ Read して、1つずつ分析する。22ファイルのテストプロジェクトで試したところ、**体感で数分かかった**。ツール呼び出し1回ごとにAPIラウンドトリップが発生するので、ファイル数に比例して遅くなる。

#### 解決策: 4つの速度最適化

**A. ツール呼び出しの並列化**

Claude Code は1メッセージで複数のツールを同時に呼び出せる。Read を22回逐次実行する代わりに、1メッセージで22個の Read を並列発行する。

```
Before: Read → 分析 → Read → 分析 → ... (22回)
After:  Read×22 (1メッセージ) → 一括分析
```

これだけで体感速度が大幅に改善した。SKILL.md に「**逐次 Read は厳禁**」と明記して、毎回確実に並列実行させるようにした。

**B. 2段階スキャン（Pass 1 / Pass 2）**

全ファイルを Read する前に、`wc -l` や `grep -c` で軽量メトリクス（行数・分岐数・import数）を一括取得する。この結果でファイルを分類する。

```
Pass 1（軽量スキャン）:
  - wc -l で行数を一括取得
  - grep -c で分岐数・関数数を一括取得
  - import パスから依存グラフを構築
  → 「Likely Healthy」と「Needs Detail」に分類

Pass 2（詳細分析）:
  - 全ファイルを Read（並列）
  - Likely Healthy は簡易スコアリング
  - Needs Detail は詳細分析
```

Pass 1 のメトリクスを Pass 2 で参照することで、分析の精度を保ちながら時間を短縮できた。特に循環参照の検出は、Pass 1 の import パスから機械的にグラフを構築するため、LLM が手動で追跡する必要がなくなった。

**C. サブエージェント並列分析**

15ファイル以上のプロジェクトでは、ファイルをグループに分割して **Task サブエージェント（haiku モデル）** に並列で分析させる。

```
メインエージェント (opus):
  Pass 1 メトリクス取得 → ファイル分割 → サブエージェント発行

サブエージェント A (haiku): ファイル1-9 を分析
サブエージェント B (haiku): ファイル10-22 を分析
  ↓ 並列実行
メインエージェント: 結果集約 → スコア補正 → レポート生成
```

haiku モデルを使うことで個別ファイルの分析を高速化し、メインエージェントは結果の集約と整合性チェックに専念する。

**D. テンプレート注入方式**

HTMLレポートのCSS/JS/HTML構造（約290行）を事前にテンプレートとして用意しておき、分析結果の JSON を1箇所に注入するだけで完成させる。

```javascript:report-template.html（抜粋）
var DATA = /*__CODEVITAL_DATA__*/ {} /*__END__*/;
```

Claude が毎回ゼロから HTML を生成する必要がなくなり、出力フェーズの時間が大幅に短縮された。テンプレートの再利用性も高い。

### 苦戦2: サブエージェントのスコアがおかしい

haiku サブエージェントに分析を委譲したところ、**明らかにスコアが不自然なケース**が出てきた。

```
helpers.ts:
  - 実態: 28個の小さなユーティリティ関数（各2-10行）
  - haiku の判定: Complexity = 48 (Critical)
  - 正しい判定: Complexity = 88 (Healthy)

  → haiku は「関数が多い」と「関数が複雑」を混同した
```

```
db.ts:
  - haiku の判定: Coupling = 40 (Critical)
  - 正しい判定: Coupling = 72 (Warning)

  → SQLインジェクションリスクをCoupling軸で減点していた（軸が違う）
```

#### 解決策: メインエージェントによる補正

サブエージェントの結果をそのまま採用するのではなく、メインエージェントが Pass 1 メトリクスと照合して検証する設計にした。「行数80行以下・分岐5以下の関数しかないファイルが Complexity = 48 はおかしい」と機械的に判定できる。

これは今後の課題でもある。サブエージェントの精度が上がれば補正の手間も減るが、現状は**メインエージェントの検証ステップが不可欠**。

### 苦戦3: Report モードと Quick モードの出力が被る

Report モードでHTMLを生成した後、Phase 4（結果報告）でターミナルにも詳細な結果を出力していた。Quick モードと同じようなテーブルや Top5 がターミナルに出る。これでは Quick モードの存在意義がない。

#### 解決策: Phase 4 の出力を明確に分離

```
Quick モード:
  Phase 3 でターミナルに詳細出力 → Phase 4 は追加報告なし

Report モード:
  Phase 3 でHTML生成 → Phase 4 は3行のサマリーのみ
```

```
CodeVital: 79/100 (Warning) — 22ファイル分析 (TypeScript)
最重要: orderHandler.ts の processOrder 関数（250行・9パラメータ）にビジネスロジックが集約
レポート: codevital-report-2026-02-19.html
```

詳細は HTML を見てもらう。シンプルだが、この「当たり前の分離」に気づくまでにテストを繰り返した。

### 苦戦4: 差分分析の設計

毎回フル分析するのは遅い。変更したファイルだけ再分析したい。しかし「前回のデータ」をどこに保存するかが問題になった。外部DBやファイルを別途管理すると、スキルの可搬性が下がる。

#### 解決策: HTMLレポートにデータを埋め込む

生成済みHTMLの中にJSON（分析データ・gitコミットハッシュ）を埋め込み、次回実行時にそこからデータを抽出する。

```bash
# HTMLからDATA JSONを抽出
sed -n 's/.*\/\*__CODEVITAL_DATA__\*\/ \(.*\) \/\*__END__\*\/.*/\1/p' codevital-report-*.html
```

```bash
# 前回コミットとの差分を取得
git diff --name-only <前回のgit_commit> HEAD -- .
```

変更がなければ「前回分析から変更はありません」と即座に終了。変更があれば Modified ファイルだけ再分析し、Unchanged ファイルは前回スコアを引き継ぐ。追加のDBや設定ファイルなしで差分分析が実現できた。

## 現状の限界と懸念

正直に書く。

### 1. スコアの客観性

最大の懸念は**スコアの再現性**。同じコードを2回分析して同じスコアが出る保証はない。LLM の出力は確率的であり、「認知的複雑度」「意味的結合」といった定性的な指標は特にブレやすい。

現状の対策は、定量指標（行数・分岐数・import数）を Pass 1 で機械的に計測し、定性判断の比重を下げること。だが根本的な解決にはなっていない。

### 2. 大規模プロジェクトへの対応

80ファイル以上のサンプリング分析は設計したが、実際にテストしたのは22ファイル程度の小規模プロジェクトのみ。100〜500ファイル規模のプロジェクトでの精度・速度は未検証。

### 3. サブエージェントの品質管理

haiku モデルのサブエージェントは速いが、前述のように誤判定がある。メインエージェントの補正に依存しているが、メインエージェント自体も間違う可能性はある。

### 4. 言語ごとの精度差

TypeScript/JavaScript では十分に機能するが、Tier 2 言語（Rust, Go, Java 等）でのテストが不足している。言語固有のパターン（Rust の `match` アーム、Go のエラーハンドリング `if err != nil` 等）を適切にスコアリングできるかは検証が必要。

## 今後の計画

### 短期: スコア精度の改善

- 複数プロジェクト（OSS含む）での検証を行い、スコアの妥当性を確認
- 閾値の調整（現状の数値は設計時の仮説ベース）
- サブエージェントへのプロンプト改善（誤判定パターンの事例をプロンプトに含める）

### 中期: CI/CD 連携

プルリクエスト単位で自動実行し、「このPRでスコアが5点下がった」と報告する仕組み。Claude Code の Hooks を使えば、コミット時に自動で Quick モード分析を走らせることも可能。

### 長期: SaaS 化の検討

ブラウザから使えるWebサービスとして公開する構想がある。ただし、コードをサーバーに送信するセキュリティ面の懸念や、API コストの採算性など、クリアすべき課題は多い。

## スキル設計で得た教訓

CodeVital の開発を通じて、Claude Code Skills の設計パターンについていくつかの教訓を得た。

### SKILL.md は「オーケストレーター」に徹する

診断基準や出力仕様の詳細を SKILL.md に全部書くと、プロンプトが肥大化してClaude の注意が分散する。SKILL.md には手順の流れだけを書き、詳細は `references/` に分離する。

```
SKILL.md:   「Pass 1 で軽量メトリクスを取得する」
            → 詳細は references/analysis-criteria.md を参照

SKILL.md:   「HTMLテンプレートのDATA部分を置換する」
            → テンプレートは references/report-template.html を参照
```

### 「やるな」を明記する

Claude は親切なので、頼んでないことをやりがちだ。CodeVital では「コードの修正は一切行わない」「逐次 Read は厳禁」「テーブルや Top5 リストは出さない（Report モードの Phase 4）」のように、**禁止事項を明示的に書く**ことで出力を制御している。

### テンプレート+データ注入は速い

HTMLをゼロから生成させると、毎回CSS/JSの出力にトークンを消費する。テンプレートを用意しておき、JSON だけ注入する方式は出力速度の点で圧倒的に有利。他のスキルでも使えるパターンだと思う。

## まとめ

CodeVital は「AIにコードの品質を意味レベルで診断させる」実験的なスキル。

- **3軸評価**: Complexity (40%) / Coupling (30%) / Responsibility (30%)
- **2つの出力**: Quick（ターミナル）/ Report（HTML）
- **速度最適化**: 並列Read、2段階スキャン、haiku サブエージェント、テンプレート注入、差分分析
- **課題**: スコア再現性、大規模対応、サブエージェント精度、多言語対応

完成度としてはまだ v0.1 という印象。特にスコアの客観性とサブエージェントの精度は改善の余地が大きい。

:::message
CodeVital を試してみた方、フィードバックをいただけると非常に助かります。

- スコアの妥当性（甘すぎる？厳しすぎる？）
- 足りない診断軸やほしい機能
- こういうプロジェクトで試してほしい、等

コメント欄か X（[@s4kura_and_miku](https://x.com/s4kura_and_miku)）で教えてください。
:::
